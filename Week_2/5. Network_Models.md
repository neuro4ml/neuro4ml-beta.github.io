# Network Models

[Downlod the slides here](W2-V4-network-models.pptx)

:::{iframe} https://www.youtube.com/embed/iH8lgzVOVMc
:width: 100%
:::

## Introduction

In this section we're going to start looking at modelling networks, which in some sense is the topic of the rest of the course.

A key question we might want to answer is: what can networks do that single neurons can’t? The simple answer is that they can do anything, which makes it tricky to decide what to include in a short section like this.
Let’s first unpack what we mean by that.

## Universal Function Approximation

From the [history in neuroscience](history) section, you’ll know that as early as 1943 McCulloch and Pitts showed that spiking neural networks could implement any logical function. And we don’t need to limit ourselves to logical functions. Just as for artificial neural networks, spiking neural networks are universal function approximators. That is, they can approximate any reasonable function.
This has been proven in various ways by different people over the years, but we're going to highlight a particularly nice and simple proof by Iulia Comsa at Google Research.
She gave a great talk about this at a conference Dan co-organised called [SNUFA](https://snufa.net ), which stands for “spiking neural networks as universal function approximators. If you like this course, you might find the research presented there interesting too, and it’s all available to watch on [YouTube](https://youtu.be/rk9AJ0w1mrw?si=_VlRXOQInIaWWMnY&t=1175).
Her [paper](https://arxiv.org/abs/1907.13223) starts by showing that you can implement the less than and greater than operators using spiking neurons. We’ll do a similar thing in this week’s exercise.

```{figure} network-modelsPicture1.png
:width: 400px
:align: center
```

Next, by adding in a network that implements logical-AND, you can express intervals or, in higher dimensions, hypercubes.

```{figure} network-modelsPicture2.png
:width: 500px
:align: center
```

Finally, by combining these you can approximate any function to as much accuracy as you want by just dividing into smaller and smaller hypercubes.
OK, so that’s great and perhaps not unexpected given that our brains are just spiking neural networks, and we like to think we’re fairly capable.
But it does mean that there’s no way we can cover all the things we can do with networks. So instead, what we're going to go through, in detail, one particular type of network, and show how it ties in to some of the concepts that neuroscientists use to understand the brain.

```{figure} network-modelsPicture3.png
:width: 400px
:align: center
```

## Sound Localisation Circuit

The circuit we’ll look at is the sound localisation circuit. One of the key ways we can tell which direction a sound is coming from is because there’s an arrival time difference of the sound between the two ears. Depending on head size and the angle the sound is arriving from, for humans this is a maximum of around 650 microseconds, we’re able to discriminate sounds arriving with a time difference as small as 20 microseconds. Some animals like owls can discriminate differences as small as just a few microseconds. So how can we detect this?

```{figure} network-modelsPicture4.png
:width: 200px
:align: center
```

One model is due to Lloyd Jeffress in 1948 and still found in textbooks, although the reality is a bit more complicated as we’ll see.
In his model, you have an array of what are sometimes called **coincidence detector neurons**. These are neurons that will only fire a spike if they receive a simultaneous input from both their synapses.
The signal arrives from the left ear, and travels at constant speed along this pathway, arriving at each neuron with a slightly different delay.
At the same time, the signal arrives from the right ear and does the same thing but in the reverse direction. When the acoustic delays exactly cancel with the neural delays, the signal will arrive at the coincidence detector at the same time and cause it to spike, and we can use the identity of which neuron fired a spike to tell us where the sound came from.

```{figure} network-modelsPicture5.png
:width: 500px
:align: center
```

In this [example](eg-sound), you can see that the sound arrives first at the left ear in the third window, and then in the right ear in the 5th window, so there’s a 2 window time difference. When the sound happens, the signal will appear at the left and start to move right, and then at the right move left. They arrive at the relevant cell at the same time causing it to light up. 

```{figure} network-modelsPicture6.png
:label: eg-sound
:width: 250px
:align: center
```

So that's how Jeffress’ model of sound localisation works, and it has a few interesting and more general features that we can dig into.

## Coincidence Detection and Tuning Curves

The first feature is coincidence detection. It’s actually quite a general property of leaky neurons, precisely because of the leak, as you can see in this [picture](coincidence-detection-graph).
When two spikes arrive at similar times, they’re enough to drive the neuron over threshold and cause a spike.
Whereas when they arrive with a larger time difference, the leak between the spikes means the peak is lower and not enough to drive it over threshold.

```{figure} network-modelsPicture7.png
:label: coincidence-detection-graph
:width: 700px
:align: center
```

We can plot the effect of this on a neuron that receives two spikes, and to make it a bit more realistic, a fluctuating, Gaussian background noise.

```{figure} network-modelsPicture8.png
:label: noisy-effect-graph
:width: 350px
:align: center
```

On the [graph](noisy-effect-graph), you can see that the neuron fires more spikes when the arrival time difference between the spikes is smaller - precisely what we want.
This is one example of what’s called a **tuning curve** in neuroscience. It shows the response of a neuron, averaged over several repetitions, to a stimulus defined by one or more variables. In this case, the variable is the arrival time difference between the two spikes, but it could be anything, like the contrast of a visual image, the amplitude or pitch of a sound etc. It can also be 2D in which case you’d plot it as an image. We’ll come back to tuning curves a bit later.

```{figure} network-modelsPicture9.png
:width: 350px
:align: center
```

## Spatial Structure

Another aspect of this network that you see many times in neuroscience is the spatial structure.
The cells [here](cell-arrangement) are in a 1 dimensional array in order of their preferred difference in arrival time.
You see this sort of pattern in 1D and 2D across the brain in auditory, visual and other modalities.

```{figure} network-modelsPicture10.png
:label: cell-arrangement
:width: 500px
:align: center
```

[Here’s](rat-barrel-cortex) an example from the barrel cortex of the rat. That’s the part of the brain that processes inputs from the rats’ whiskers. The coloured blobs shows the preferred stimulation direction of the cell recorded at that position. The spatial structure isn’t obvious because there are so few cells recorded.

```{figure} network-modelsPicture11.png
:label: rat-barrel-cortex
:width: 250px
:align: center

The barrel cortex of a rat
```

But, you start to see it if you combine across multiple animals.

```{figure} network-modelsPicture12.png
:label: multi-barrel-cortex
:width: 250px
:align: center

The combined barrel cortex of multiple animals
```

And with a bit of smoothing it becomes even clearer.

```{figure} network-modelsPicture13.png
:label: smooth-barrel-cortex
:width: 250px
:align: center

Smoothed barrel cortex of multiple animals
```

One interesting feature of this is that it’s learned, as you don’t see this in young rats, only adults.
In terms of modelling, it’s fairly easy to just add some metadata to each neuron specifying its position, and use that in your models. For example, to make connections between nearby neurons more likely than between neurons that are far away.

## Encoding and Decoding

With the idea of tuning curves and spatial structure in mind, we can talk about how information is encoded and decoded in this network.
Essentially, the model Jeffress proposed is what we would now call a 1-hot encoding. Knowing the identity or index of the most active neuron tells you the category of the data. In this case, a value put into one of several discrete intervals.
More recently, it was noted that in the presence of neural noise, this isn’t a very robust way of decoding that information.

```{figure} network-modelsPicture14.png
:width: 500px
:align: center
```

David McAlpine and colleagues proposed an alternative view of what the brain might be doing to decode information from this network. Their idea was to take the difference of the summed activities of the cells that prefer right leading sounds to those that prefer left leading sounds, and to use this 1d variable to regress the location. It turns out this is much more robust to noise. Conceptually, this is because by summing or equivalently averaging all the activity of the cells, you reduce the effect of noise.

```{figure} network-modelsPicture15.png
:width: 500px
:align: center
```

In 2013, Dan wrote a [paper](https://elifesciences.org/articles/01312) which can be summarised by saying that you can do better than either of these approaches by using a standard multi-class perceptron. This makes use of all the information in a way that neither of the other two models does, including averaging out the noise. In one of those strange twists of fate, the same year Dan's paper came out, another [paper](https://doi.org/10.1523/JNEUROSCI.2034-13.2013) came out from the lab Dan had just moved to that was basically the same, only they used a Bayesian decoding framework rather than perceptrons, and so we’ll use their paper to talk about that framework.

```{figure} network-modelsPicture16.png
:width: 350px
:align: center
```

## Maximum Likelihood and Bayesian Estimators