# STDP

[Downlod the slides here](W4-V2-STDP.pptx)

:::{iframe} https://www.youtube.com/embed/fvzzwHKlMzk
:width: 100%
:::

## Introduction

Spike Timing Dependent Plasticity (STDP) is a form of [Hebbian learning](hebbian) that takes the timing of individual spikes into account.

## Experimental Measurement

Let's start with how it's measured. Find a pair of neurons connected by a [synapse](#chemical-synapses). Force the pre-synaptic neuron to fire a spike at time 0 and the post-synaptic neuron to fire a spike at time $\Delta t$. We then repeat this pairing about 60 times, wait around an hour, and measure the change in the synapse. This is measure by computing the height of a post-synaptic current from a single excitatory spike before and after the pairing.

```{figure} stdpPicture1.png
:align: center
:width: 350px
```

And we get result that look like [this](stdp-results-graph). Itâ€™s a bit noisy, but you can generally see that if the post-synaptic neuron fires after the pre-synaptic neuron, the weight goes up. If the post-synaptic neuron fires before the pre-synaptic neuron, the weight goes down. The closer in time, the stronger the effect.

```{figure} stdpPicture2.png
:label: stdp-results-graph
:align: center
:width: 400px
```

We can roughly fit this with a [pair of exponentials](stdp-exp-graph) with different heights and time constants. We talked before about Hebbian learning (what fires together, wires together) but this is actually closer to what Hebb originally suggested. He suggested that when a pre-synaptic neuron repeatedly contributes to the firing of a post-synaptic neuron, the synapse will get stronger. STDP realises that idea because if the post-synaptic neuron fires before the pre-synaptic neuron, of course it couldnâ€™t be that the pre-synaptic neuron contributed to that firing. Similarly, if the timing of the spikes is far apart, itâ€™s unlikely they were related. So far, this looks like a very neat story of discovering experimental evidence for an intuitive theoretical idea and then finding a simple way to fit the data. But, the brain never makes it that easy on us.

```{figure} stdpPicture3.png
:label: stdp-exp-graph
:align: center
:width: 400px
```

```{math}
\Delta w = 
\begin{cases}
    A_+ e^{-\frac{\Delta t}{\tau_+}} \quad \text{if } \Delta t > 0 \\
    A_- e^{+\frac{\Delta t}{\tau_-}} \quad \text{if } \Delta t < 0
\end{cases}
```

As well as finding the shape that Hebb would have predicted, we also find the opposite, as well as synapses that get stronger if the spikes are close in time regardless of sign, and so on.

```{figure} stdpPicture4.png
:align: center
:width: 300px
```

With that said, the pre before post shape is the one weâ€™re normally talking about, and that what we'll focus on in this section.

## Modelling STDP

So how do we model this? The simplest thing would be just to sum the weight change over all spike pairs. The issue is that this is computationally expensive. Suppose we have $N$ neurons all-to-all connected to $N$ neurons, and each neuron fires around $R$ spikes. In that case, computing this sum takes around $N^2$ $R^2$ operations, and each operation contains a call to the exponential function which is itself very heavy.

```{math}
\text{Single pair weight change:} \\
\Delta w = F(\Delta t) =  
\begin{cases}
    A_+ e^{-\frac{\Delta t}{\tau_+}} \quad \text{if } \Delta t > 0 \\
    A_- e^{+\frac{\Delta t}{\tau_-}} \quad \text{if } \Delta t < 0
\end{cases} 

\\[10pt]

\text{All pairs weight change}\\
w \leftarrow w + \sum_i \sum_j F(t_j^{post} - t_i^{pre})
```

There is a trick - using the fact that itâ€™s exponentials and linear sums, that can simplify this. For each pre-synaptic neuron we introduce what is called a **trace variable**, $a_{pre}$. We do the same with the post-synaptic neurons and a trace variable $a_{post}$.

We update them according to these rules. In the absence of a spike, they decay exponentially like weâ€™ve seen in the leaky integrate-and-fire neuron. The pre-synaptic trace with time constant $\tau_+$ and the post-synaptic with time constant $\tau_-$. When a pre-synaptic spike arrives, $a_{pre}$ increases by $A_+$ and then $w$ is increased by $a_{post}$. And similarly for when a post-synaptic spike arrives but with pre and post swapped and + and - swapped. 

```{math}
\textcolor{#0a76f2}{\text{No Spike}} \\
\tau_+ a_{pre}' = -a_{pre} \\
\tau_- a_{post}' = -a_{post} \\
``` 

```{math}
\textcolor{#0a76f2}{\text{Pre-synaptic Spike}} \\
a_{pre} \leftarrow a_{pre} + A_+ \\
w \leftarrow w + a_{post} \\
``` 

```{math}
\textcolor{#0a76f2}{\text{Post-synaptic Spike}} \\
a_{post} \leftarrow a_{post} + A_- \\
w \leftarrow w + a_{pre} \\
``` 

:::{tip} To DoðŸŽ¯
Itâ€™s a good exercise to work out why this gives the same result. Try checking what happens for a single pair of spikes first, say a pre-synaptic spike at time 0 and a post-synaptic spike at time $\Delta t > 0$. Draw a plot of what happens to both $a_{pre}$ and $a_{post}$ and at each spike time, what happens to $w$. Now do the same when the post-synaptic spike happens first. This gives you the single pair weight change equation at the top, and if you use the fact that all equations are linear to sum over all spike pairs, you get the all pairs weight change rule.
:::

Computationally, we now only have to do $N^2 R$ operations, and theyâ€™re all arithmetic instead of exponential. This is essentially free because we had to this many operations anyway. For each spike at a pre-synaptic neuron, we have to add a value to each post-synaptic neuron itâ€™s connected to. There are $N$ neurons affected by each spike, and there are $NR$ pre-synaptic neurons, so thatâ€™s $N^2 R$ operations we would have had to do anyway.

One thing thatâ€™s worth pointing out is that the models slightly differ from the experimental data here. In the experimental data, it takes around an hour before you see the weight change, with it slowly increasing over time. With the first implementation, we could run the weight change rule an hour after the spikes we want to learn, but even that doesnâ€™t match the slow change you see in experiments. With the second implementation, itâ€™s automatically updated immediately after each spike.

Itâ€™s also worth noting that there are some subtleties with delays here. You should do a slightly different thing depending on whether the delays are dendritic, axonal, or some combination of the two. In any case, the experimental data is less clear about delays, so itâ€™s not even obvious that there is a right thing to do here.

## Latency Reduction

Now that we know how to model STDP, letâ€™s start taking a look at the things it can learn.
We set up a model with a layer of input neurons connected to a single output neuron, with STDP on the synaptic weights. Now, we make those input neurons all fire a burst of spikes for 20 ms but with a random latency, and look at what the network learns.

```{figure} stdpPicture5.png
:align: center
:width: 700px
```

What we see is that the neurons with a low latency have their synaptic weights strengthened to the maximum level, and those with a high latency drop to zero. The output neuron now spikes earlier than before learning. One way of interpreting this would be that if there were multiple sources of information available to an animal, it would respond preferentially to the one that arrived earliest. This would clearly be advantageous in an environment where a short delay in responding could be the difference between life and death for an animal being hunted.

```{figure} stdpPicture6.png
:align: center
:width: 700px
```

## STDP Learns Correlated Groups

In this next experiment, we divide input neurons into two groups. One group with the white background are firing uncorrelated spikes. The other group with the blue background are firing correlated spikes.

```{figure} stdpPicture7.png
:align: center
:width: 700px
```

After STDP, the weights of the neurons in the uncorrelated group go to zero, while the weights in the correlated group go to the maximum value. This makes sense biologically because being able to pick up on input correlations in your environment is very useful.

```{figure} stdpPicture8.png
:align: center
:width: 300px
```

We can do the same thing but where both groups of neurons are correlated with other neurons in their group, but not with neurons in the other group. We find that STDP will learn to pick one of the two groups, but which one is random. In other words, STDP has some aspects of competition in what it learns. You can think of this as a bit like the [selectivity](BCM-selectivity) we talked about with the [BCM rule](#BCM-Rule) in the previous section.
